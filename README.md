# Production-Grade Multi-Tier Web App Infrastructure on AWS using CDK & CLI Only

## Prerequisites

### Set up billing alarm (Three billing alarms: One using console and two using CLI)
Before I started building, I set up billing alarms with AWS CLI and SNS so I wouldn’t accidentally overspend. I can monitor my estimated AWS charges by using Amazon CloudWatch. Billing metric data is stored in the US East (N. Virginia) Region and represents worldwide charges and the data includes the estimated charges for every service that I use.
Before I can create an alarm, I must enable billing alerts, so that I can monitor my estimated AWS charges and create an alarm using billing metric data. It takes about 15 minutes before one can view billing data and set billing alarms.

I created two guardrails in my AWS account by deploying two CloudWatch Billing Alarms. I created the $5 'SafetyNetBuffer' alarm using the graphical Console, providing a high level alert for moderate costs. Then, I followed up by creating a tighter $1 threshold alarm using AWS CLI. This dual approach not only ensures I get an early warning about any unexpected spending spikes (the $1 alert) but also teaches a valuable skill of using the CLI for automation.

---

## Create a dedicated IAM user

For this project, I decided to create a dedicated IAM user who deploys the infra and create the user using the principle of least privilege. So to make things reusable, i created a script create_iam_user.s. I first laid out a mental model on the necessary AWS resources that the IAM user needs to interact to and based on that model, I created a custom IAM policy named 3-tier-deployment-policy. To enable this new user to perform deployments via CLI, I used the aws iam create-access-key command to generate a long-term Access Key ID and Secret Access Key. Instead of exposing these secrets in the script, I stored them in a local AWS CLI named profile (3-tier-infra-user-profile). After this, I finally attached the custom policy to the user.

**Error Debugging:**  
When capturing the output of a shell function into a variable using command substitution ($()), it's crucial to remember that the shell captures everything sent to Standard Output (stdout). A common error occurs when diagnostic messages, intended only for the user, are accidentally captured alongside the intended return value. In my case, the tee command within the log function was the culprit: tee writes to both the log file and stdout by default. When the function used echo "$policy_arn" to return the policy arn, the stdout from tee was also prepended to it, creating a multi-line string that the AWS CLI rejected as an "Invalid ARN."  
**Fix:** Modify the log function to redirect stdout generated by tee to stderr (>&2).

---

## Writing the entire infra in AWS CDK

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation. Before using CDK, I must perform bootstrapping.

```bash
cdk bootstrap aws://<account-id>/<region>
```

This command creates a special CloudFormation Stack in my account called something like **CDKToolkit**, which contains an S3 bucket, IAM roles, etc. Later, when I run `cdk deploy`, the CDK CLI generates a CloudFormation template, uploads all assets to that S3 bucket/ECR repo, and tells CloudFormation to deploy.

To deploy the infra via CDK, I divided the project into:
- **Constructs** → The building blocks (e.g., VpcConstruct, AsgConstruct)
- **Stacks** → High-level grouping of constructs
- **App** → The entry point defining deployment order and environment

Deployment:
```bash
cdk deploy --all
```

---

## Network Tier

A single VPC is created that acts like a private cloud in AWS. Using CDK, I created 6 subnets (3 in each of 2 AZs) for:
- Separation of concerns (public vs. private resources)
- Security (databases isolated from the internet)
- High Availability (multiple AZs)

Public subnets → ALB + NAT Gateway  
Private subnets → EC2 + RDS

CDK automatically attaches Internet Gateway and configures route tables when using `ec2.Vpc` construct.

---

## Web Tier

The **Application Load Balancer (ALB)** acts as the entry point for incoming traffic (port 80).  
- Security group: allows inbound on port 80 only  
- Distributes traffic to **Target Group** on port 8080  
- Health checks on "/"  

---

## App Tier

The **Auto Scaling Group (ASG)** lives in private subnets, linked to the ALB.  
- Uses **Amazon Linux 2** AMI  
- IAM Role: permissions for SSM, CloudWatch Logs/Metrics, Secrets Manager  
- Security: Only ALB (port 8080) and RDS (port 5432) allowed  
- **Scaling** based on CPU utilization  

For **CloudWatch Agent**, configuration JSON stored in **SSM Parameter Store**, fetched during startup.

---

## Database Tier

**PostgreSQL RDS** in **PRIVATE ISOLATED subnets**.  
- No internet access  
- Access controlled via Security Groups (ASG → RDS on port 5432)  
- Credentials in **AWS Secrets Manager**  
- Application IAM Role reads secret dynamically  

---

## Manage everything through AWS CLI and Shell Scripts

I wrote multiple AWS CLI scripts for:
- Checking instance & target health  
- Retrieving RDS/EC2 info  
- SSH via SSM  
- Getting app logs  

**EC2 Metadata** (169.254.169.254) used for self-discovery of instance info.

---

## Implement Observability

- **CloudWatch Agent** for logs and metrics (CPU, disk, NGINX logs, custom logs)
- **CloudWatch Alarms** for:
  - EC2 CPU > 80%
  - RDS storage nearing limit
